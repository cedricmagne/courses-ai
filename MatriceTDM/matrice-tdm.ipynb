{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3191236f",
   "metadata": {},
   "source": [
    "# Setup de l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be308f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\web\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Base comparative méthode Reinert: https://cran.r-project.org/web/packages/rainette/vignettes/algorithmes.html\n",
    "# Etude de la présence/absence inter documentaire\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_en = stopwords.words()\n",
    "stop_words_fr = stopwords.words(\"french\")\n",
    "stop_words = stop_words_en + stop_words_fr\n",
    "# Dépendance de punkt pour le modèle de tokenisation\n",
    "nltk.download(\"punkt_tab\")\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "from collections import Counter\n",
    "from spacy.symbols import NOUN, VERB, ADJ, ADV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26a9a",
   "metadata": {},
   "source": [
    "# Import du fichier de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7013ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reponse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quelle définition donneriez-vous de l'opinion ...</td>\n",
       "      <td>La définition de l’opinion publique, selon moi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Que pensez-vous de l'influence des outils digi...</td>\n",
       "      <td>Je dirais que l’influence, enfin je dirais tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Est-ce que vous arrivez à récolter facilement ...</td>\n",
       "      <td>Il y a un élément qui nous le rappelle, qui no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pourriez-vous vous passez d'un site internet d...</td>\n",
       "      <td>Moi je pense, c’est ce que j’ai dit tout à l’h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Est-ce que vous participez, ou avez participé,...</td>\n",
       "      <td>Oui bien sûr, j’ai déjà participé à des webcam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Est-ce que la base militante est difficile à m...</td>\n",
       "      <td>Ça dépend des élections. Ça dépend du contexte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Que pensez-vous de l'utilisation des plateform...</td>\n",
       "      <td>En fait j’ai toujours trouvé ça, mais c’est un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Est-ce que vous pensez que les boucles sont un...</td>\n",
       "      <td>Oui, je pense que les boucles sont une bonne s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Comment se déroule la planification des action...</td>\n",
       "      <td>C’est très, très divers. C’est un peu dans le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Pensez-vous que les outils numériques puissent...</td>\n",
       "      <td>Alors oui je pense tout à fait que c’est une b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Quelle définition donneriez-vous de l'opinion ...   \n",
       "1    Que pensez-vous de l'influence des outils digi...   \n",
       "2    Est-ce que vous arrivez à récolter facilement ...   \n",
       "3    Pourriez-vous vous passez d'un site internet d...   \n",
       "4    Est-ce que vous participez, ou avez participé,...   \n",
       "..                                                 ...   \n",
       "191  Est-ce que la base militante est difficile à m...   \n",
       "192  Que pensez-vous de l'utilisation des plateform...   \n",
       "193  Est-ce que vous pensez que les boucles sont un...   \n",
       "194  Comment se déroule la planification des action...   \n",
       "195  Pensez-vous que les outils numériques puissent...   \n",
       "\n",
       "                                               reponse  \n",
       "0    La définition de l’opinion publique, selon moi...  \n",
       "1    Je dirais que l’influence, enfin je dirais tou...  \n",
       "2    Il y a un élément qui nous le rappelle, qui no...  \n",
       "3    Moi je pense, c’est ce que j’ai dit tout à l’h...  \n",
       "4    Oui bien sûr, j’ai déjà participé à des webcam...  \n",
       "..                                                 ...  \n",
       "191  Ça dépend des élections. Ça dépend du contexte...  \n",
       "192  En fait j’ai toujours trouvé ça, mais c’est un...  \n",
       "193  Oui, je pense que les boucles sont une bonne s...  \n",
       "194  C’est très, très divers. C’est un peu dans le ...  \n",
       "195  Alors oui je pense tout à fait que c’est une b...  \n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import du fichier\n",
    "dataset_path = \"files/ph-th-entretiens.xlsx\"\n",
    "df = pd.read_excel(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a25d2",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8eecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation des regex en constantes\n",
    "DIGITS_PATTERN = re.compile(r'\\d')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "PUNCTUATION_TRANS = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "def uci(value):\n",
    "    \"\"\"\n",
    "    Nettoie une chaîne de caractères en supprimant les chiffres, accents,\n",
    "    ponctuations et stop words.\n",
    "    \n",
    "    Args:\n",
    "        value (str): Chaîne à nettoyer\n",
    "        \n",
    "    Returns:\n",
    "        str: Chaîne nettoyée\n",
    "    \"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return \"\"\n",
    "        \n",
    "    try:\n",
    "        # Suppression des chiffres et conversion en minuscules\n",
    "        value = DIGITS_PATTERN.sub('', value.lower())\n",
    "        \n",
    "        # Suppression des accents\n",
    "        value = unidecode(value)\n",
    "        \n",
    "        # Suppression de la ponctuation et des espaces multiples\n",
    "        value = WHITESPACE_PATTERN.sub(' ', value.translate(PUNCTUATION_TRANS).strip())\n",
    "        \n",
    "        # Filtrage des mots\n",
    "        return ' '.join(\n",
    "            word for word in value.split()\n",
    "            if len(word) >= 3 and word not in stop_words\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du nettoyage UCI: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f3a39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Début du traitement des UCI...\n",
      "INFO:__main__:Statistiques des métadonnées trouvées:\n",
      "INFO:__main__:Traitement des UCI terminé.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Exemple des 5 premières UCI avec leurs métadonnées:\n",
      "UCI: definition opinion publique selon instant pense public vis vis sujet occurrence opinion publique politique pense comment positionne gens quand dis gens grand public peut etre associations chefs entreprise donc definition donne opinion public comment opinion public mesure taux satisfaction regard question posee\n",
      "\n",
      "UCI: dirais influence enfin dirais tout simplement reseaux sociaux digital presents communication politique autant pris part importante communication politique parce comme certains disent faut vivre temps neanmoins faut savoir mesurer veux dire campagne politique evidemment reseaux sociaux evidemment numerique rien mieux aussi contact gens mesurer tout cela dire outil numerique vient complementarite autres outils evidemment rappele pandemie accelere impose modifie relation pouvait avoir puisqu campagne coller affiches contact distribuer tracts marches rencontre individus quel pendant periode covid faire donc dire partie numerique existait pris part importante obligation moment donne neanmoins faut savoir mesurer tout cela autant faut vivre outils numeriques autant faut mettre tete outil numerique outils seul meme aujourd imagine campagne electorale guidee aussi outil numerique\n",
      "\n",
      "UCI: element rappelle rappelle souvent autant taux abstention quand voit taux aussi eleve taux abstention existait neanmoins accru petit peu vais dire entrer contact parce arrive entrer contact gens neanmoins opinion publique desinteret grandissant concernant politique alors assez paradoxal parce quand rencontre gens disent politique interesse sujets politiques interessent guillemets personnel politique peut dire ainsi donc besoin pays interesse beaucoup chose publique voit neanmoins comment redonne sens politique sens large parce fonction politique decredibilisee\n",
      "\n",
      "UCI: pense tout heure pense puisse peut passer apres comment fait vivre parce site campagne essentiel important faut vivre temps neanmoins rien pire avoir site campagne vivant quand dis vivant actualise permette lien parce toujours lien campagne electorale lien lien distance lien presentiel lien physique neanmoins mettre site ligne campagne obligation reel besoin neanmoins faut quelqu alimenter faire vivre donc faut trompe quand fait site campagne fois vaut mieux rien avoir avoir site vit parce politique vie aussi relation echanges echange fait besoin etre nourri evidemment site alors site outils numeriques essentiels aujourd mener campagnes neanmoins oublier faire vivre sites alimenter nourrir sites interactifs permettent contact population\n",
      "\n",
      "UCI: oui participe webcams rencontres autant redis autant situation sanitaire laquelle ete accelere participe maniere generale communication evidemment faut etre lien outils outils numeriques existent tres principalement presse regionale locale aussi outils numeriques donc participe cela autre part interne reunions campagne reunions politiques fait genre echanges\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des UCI : unité de contexte initial\n",
    "# Tagging des métadonnées : Le début de l'énnoncé est marqué par *NomVariable\"  ex : *Partie_l *chapitre_l_l\n",
    "# Création des UCI avec gestion complète des métadonnées\n",
    "import logging\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Compilation du pattern pour les métadonnées\n",
    "METADATA_PATTERN = re.compile(r'(\\*[A-Za-z_]+\\d*)')\n",
    "\n",
    "# Création des UCI avec métadonnées\n",
    "def process_uci_with_metadata(text):\n",
    "    \"\"\"\n",
    "    Traite le texte en préservant les métadonnées et en appliquant le nettoyage UCI\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à traiter\n",
    "    Returns:\n",
    "        str: Texte nettoyé avec métadonnées préservées\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Extraction des métadonnées\n",
    "    metadata = METADATA_PATTERN.findall(text)\n",
    "    \n",
    "    # Nettoyage du texte principal\n",
    "    cleaned_text = uci(text)\n",
    "    \n",
    "    # Réinsertion des métadonnées au début\n",
    "    if metadata:\n",
    "        return ' '.join(metadata) + ' ' + cleaned_text\n",
    "    return cleaned_text\n",
    "\n",
    "# Application du traitement sur le DataFrame\n",
    "logger.info(\"Début du traitement des UCI...\")\n",
    "df['uci'] = df['reponse'].astype(str).map(process_uci_with_metadata)\n",
    "\n",
    "# Création des colonnes séparées pour analyse\n",
    "df['metadata'] = df['uci'].str.extract(r'^((?:\\*[A-Za-z_]+\\d*\\s*)+)')\n",
    "df['clean_text'] = df['uci'].str.replace(r'^\\s*(?:\\*[A-Za-z_]+\\d*\\s*)+', '')\n",
    "\n",
    "# Analyse des métadonnées\n",
    "metadata_stats = df['uci'].str.extractall(r'(\\*[A-Za-z_]+\\d*)').value_counts()\n",
    "\n",
    "# Affichage des statistiques\n",
    "logger.info(\"Statistiques des métadonnées trouvées:\")\n",
    "print(metadata_stats)\n",
    "\n",
    "# Affichage des premiers résultats\n",
    "print(\"\\nExemple des 5 premières UCI avec leurs métadonnées:\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    metadata = METADATA_PATTERN.findall(row['uci'])\n",
    "    if metadata:\n",
    "        print(f\"Row {idx} - Metadata: {metadata}\")\n",
    "    print(f\"UCI: {row['uci']}\\n\")\n",
    "\n",
    "logger.info(\"Traitement des UCI terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39063f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Début du traitement des UCE...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\web\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "INFO:__main__:Analyse des résultats UCE...\n",
      "INFO:__main__:Traitement des UCE terminé.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple des 5 premiers résultats UCI → UCE:\n",
      "\n",
      "Row 0:\n",
      "UCI: definition opinion publique selon instant pense public vis vis sujet occurrence opinion publique politique pense comment positionne gens quand dis gens grand public peut etre associations chefs entreprise donc definition donne opinion public comment opinion public mesure taux satisfaction regard question posee\n",
      "UCE: definition opinion publique selon instant pense public vis vis sujet occurrence opinion publique politique pense comment positionne gens quand dis gens grand public peut etre associations chefs entreprise donc definition donne opinion public comment opinion public mesure taux satisfaction regard question posee\n",
      "\n",
      "Row 1:\n",
      "UCI: dirais influence enfin dirais tout simplement reseaux sociaux digital presents communication politique autant pris part importante communication politique parce comme certains disent faut vivre temps neanmoins faut savoir mesurer veux dire campagne politique evidemment reseaux sociaux evidemment numerique rien mieux aussi contact gens mesurer tout cela dire outil numerique vient complementarite autres outils evidemment rappele pandemie accelere impose modifie relation pouvait avoir puisqu campagne coller affiches contact distribuer tracts marches rencontre individus quel pendant periode covid faire donc dire partie numerique existait pris part importante obligation moment donne neanmoins faut savoir mesurer tout cela autant faut vivre outils numeriques autant faut mettre tete outil numerique outils seul meme aujourd imagine campagne electorale guidee aussi outil numerique\n",
      "UCE: dirais influence enfin dirais tout simplement reseaux sociaux digital presents communication politique autant pris part importante communication politique parce comme certains disent faut vivre temps neanmoins faut savoir mesurer veux dire campagne politique evidemment reseaux sociaux evidemment numerique rien mieux aussi contact gens mesurer tout cela dire outil numerique vient complementarite autres outils evidemment rappele pandemie accelere impose modifie relation pouvait avoir puisqu campagne coller affiches contact distribuer tracts marches rencontre individus quel pendant periode covid faire donc dire partie numerique existait pris part importante obligation moment donne neanmoins faut savoir mesurer tout cela autant faut vivre outils numeriques autant faut mettre tete outil numerique outils seul meme aujourd imagine campagne electorale guidee aussi outil numerique\n",
      "\n",
      "Row 2:\n",
      "UCI: element rappelle rappelle souvent autant taux abstention quand voit taux aussi eleve taux abstention existait neanmoins accru petit peu vais dire entrer contact parce arrive entrer contact gens neanmoins opinion publique desinteret grandissant concernant politique alors assez paradoxal parce quand rencontre gens disent politique interesse sujets politiques interessent guillemets personnel politique peut dire ainsi donc besoin pays interesse beaucoup chose publique voit neanmoins comment redonne sens politique sens large parce fonction politique decredibilisee\n",
      "UCE: element rappelle rappelle souvent autant taux abstention quand voit taux aussi eleve taux abstention existait neanmoins accru petit peu vais dire entrer contact parce arrive entrer contact gens neanmoins opinion publique desinteret grandissant concernant politique alors assez paradoxal parce quand rencontre gens disent politique interesse sujets politiques interessent guillemets personnel politique peut dire ainsi donc besoin pays interesse beaucoup chose publique voit neanmoins comment redonne sens politique sens large parce fonction politique decredibilisee\n",
      "\n",
      "Row 3:\n",
      "UCI: pense tout heure pense puisse peut passer apres comment fait vivre parce site campagne essentiel important faut vivre temps neanmoins rien pire avoir site campagne vivant quand dis vivant actualise permette lien parce toujours lien campagne electorale lien lien distance lien presentiel lien physique neanmoins mettre site ligne campagne obligation reel besoin neanmoins faut quelqu alimenter faire vivre donc faut trompe quand fait site campagne fois vaut mieux rien avoir avoir site vit parce politique vie aussi relation echanges echange fait besoin etre nourri evidemment site alors site outils numeriques essentiels aujourd mener campagnes neanmoins oublier faire vivre sites alimenter nourrir sites interactifs permettent contact population\n",
      "UCE: pense tout heure pense puisse peut passer apres comment fait vivre parce site campagne essentiel important faut vivre temps neanmoins rien pire avoir site campagne vivant quand dis vivant actualise permette lien parce toujours lien campagne electorale lien lien distance lien presentiel lien physique neanmoins mettre site ligne campagne obligation reel besoin neanmoins faut quelqu alimenter faire vivre donc faut trompe quand fait site campagne fois vaut mieux rien avoir avoir site vit parce politique vie aussi relation echanges echange fait besoin etre nourri evidemment site alors site outils numeriques essentiels aujourd mener campagnes neanmoins oublier faire vivre sites alimenter nourrir sites interactifs permettent contact population\n",
      "\n",
      "Row 4:\n",
      "UCI: oui participe webcams rencontres autant redis autant situation sanitaire laquelle ete accelere participe maniere generale communication evidemment faut etre lien outils outils numeriques existent tres principalement presse regionale locale aussi outils numeriques donc participe cela autre part interne reunions campagne reunions politiques fait genre echanges\n",
      "UCE: oui participe webcams rencontres autant redis autant situation sanitaire laquelle ete accelere participe maniere generale communication evidemment faut etre lien outils outils numeriques existent tres principalement presse regionale locale aussi outils numeriques donc participe cela autre part interne reunions campagne reunions politiques fait genre echanges\n",
      "\n",
      "Statistiques de réduction:\n",
      "Moyenne mots UCI: 163.97\n",
      "Moyenne mots UCE: 163.97\n"
     ]
    }
   ],
   "source": [
    "# Définition des UCE\n",
    "# Création des UCE à partir des UCI\n",
    "import re\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Compilation du pattern pour les métadonnées\n",
    "METADATA_PATTERN = re.compile(r'(\\*[A-Za-z_]+\\d*)')\n",
    "\n",
    "def uce(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Crée les Unités de Contexte Élémentaire (UCE) à partir du texte\n",
    "    Une UCE correspond généralement à une phrase ou un segment significatif\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à traiter\n",
    "    Returns:\n",
    "        str: Texte segmenté en UCE\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Téléchargement du modèle de tokenization si nécessaire\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        \n",
    "        # Segmentation en phrases\n",
    "        sentences = sent_tokenize(text, language='french')\n",
    "        \n",
    "        # Nettoyage et filtrage des phrases\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # Nettoyage basique\n",
    "            cleaned = sentence.strip()\n",
    "            \n",
    "            # Filtres de validation\n",
    "            if (len(cleaned.split()) >= 3 and  # Au moins 3 mots\n",
    "                any(c.isalpha() for c in cleaned)):  # Au moins une lettre\n",
    "                cleaned_sentences.append(cleaned)\n",
    "        \n",
    "        # Jointure des phrases valides\n",
    "        return ' '.join(cleaned_sentences)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la création des UCE: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "def process_uce_from_uci(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Traite le texte UCI pour créer l'UCE en préservant les métadonnées\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte UCI à traiter\n",
    "    Returns:\n",
    "        str: Texte UCE avec métadonnées préservées\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Extraction des métadonnées existantes\n",
    "    metadata = METADATA_PATTERN.findall(text)\n",
    "    \n",
    "    # Application de l'UCE sur le texte sans les métadonnées\n",
    "    cleaned_text = text\n",
    "    if metadata:\n",
    "        # Retire temporairement les métadonnées pour le traitement\n",
    "        cleaned_text = METADATA_PATTERN.sub('', text).strip()\n",
    "    \n",
    "    # Application du traitement UCE\n",
    "    processed_text = uce(cleaned_text)\n",
    "    \n",
    "    # Réinsertion des métadonnées\n",
    "    if metadata:\n",
    "        return ' '.join(metadata) + ' ' + processed_text\n",
    "    return processed_text\n",
    "\n",
    "# Application sur le DataFrame\n",
    "try:\n",
    "    logger.info(\"Début du traitement des UCE...\")\n",
    "    \n",
    "    # Création des UCE\n",
    "    df['uce'] = df['uci'].astype(str).map(process_uce_from_uci)\n",
    "    \n",
    "    # Analyse des résultats\n",
    "    logger.info(\"Analyse des résultats UCE...\")\n",
    "    print(\"\\nExemple des 5 premiers résultats UCI → UCE:\")\n",
    "    for idx, row in df.head().iterrows():\n",
    "        print(f\"\\nRow {idx}:\")\n",
    "        print(f\"UCI: {row['uci']}\")\n",
    "        print(f\"UCE: {row['uce']}\")\n",
    "    \n",
    "    # Statistiques de comparaison UCI vs UCE\n",
    "    df['uci_length'] = df['uci'].str.split().str.len()\n",
    "    df['uce_length'] = df['uce'].str.split().str.len()\n",
    "    print(\"\\nStatistiques de réduction:\")\n",
    "    print(f\"Moyenne mots UCI: {df['uci_length'].mean():.2f}\")\n",
    "    print(f\"Moyenne mots UCE: {df['uce_length'].mean():.2f}\")\n",
    "    \n",
    "    logger.info(\"Traitement des UCE terminé.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors du traitement des UCE: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5102aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Début de la construction du vocabulaire...\n",
      "INFO:__main__:Statistiques du vocabulaire:\n",
      "INFO:__main__:total_lemmes: 3584\n",
      "INFO:__main__:freq_min: 1\n",
      "INFO:__main__:freq_max: 664\n",
      "INFO:__main__:freq_moyenne: 7.153180803571429\n",
      "INFO:__main__:distribution_categories: {'Nom': 1412, 'Adjectif': 1017, 'Verbe': 922, 'Adverbe': 233}\n",
      "INFO:__main__:Vocabulaire exporté vers: ./export/vocabulaire_analyse.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu du vocabulaire analysé:\n",
      "        Lemme Forme_originale Categorie  Longueur  Frequence\n",
      "22       donc            donc   Adverbe         4        664\n",
      "103     faire           faire     Verbe         5        662\n",
      "63   campagne        campagne       Nom         8        520\n",
      "19    pouvoir            peut     Verbe         7        441\n",
      "15       dire             dis     Verbe         4        391\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Construction du tableau de vocabulaire avec analyse statistique\n",
    "# --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "logger.info(\"Début de la construction du vocabulaire...\")\n",
    "\n",
    "def build_vocabulary_table(df: pd.DataFrame, column: str = 'uce', min_length: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construit un tableau de vocabulaire enrichi à partir des UCE\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame source\n",
    "        column (str): Nom de la colonne à analyser\n",
    "        min_length (int): Longueur minimale des lemmes à conserver\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame enrichi du vocabulaire\n",
    "    \"\"\"\n",
    "    # Définition des catégories grammaticales à conserver\n",
    "    POS_CATEGORIES = {'NOUN': 'Nom', 'VERB': 'Verbe', 'ADJ': 'Adjectif', 'ADV': 'Adverbe'}\n",
    "    \n",
    "    # Structures pour l'analyse\n",
    "    vocabulary_data: List[Dict] = []\n",
    "    lemma_counter = Counter()\n",
    "\n",
    "    # Traitement du corpus\n",
    "    for text in df[column].dropna():\n",
    "        doc = nlp(str(text))\n",
    "        for token in doc:\n",
    "            if (token.pos_ in POS_CATEGORIES and \n",
    "                len(token.lemma_) > min_length):\n",
    "                \n",
    "                lemma_counter[token.lemma_] += 1\n",
    "                vocabulary_data.append({\n",
    "                    'Lemme': token.lemma_,\n",
    "                    'Forme_originale': token.text,\n",
    "                    'Categorie': POS_CATEGORIES[token.pos_],\n",
    "                    'Longueur': len(token.lemma_)\n",
    "                })\n",
    "    \n",
    "    # Création du DataFrame enrichi\n",
    "    df_vocab = pd.DataFrame(vocabulary_data)\n",
    "    \n",
    "    # Ajout des statistiques\n",
    "    df_vocab['Frequence'] = df_vocab['Lemme'].map(lemma_counter)\n",
    "    df_vocab = df_vocab.drop_duplicates(subset=['Lemme'])\n",
    "    \n",
    "    # Tri et organisation\n",
    "    df_vocab = df_vocab.sort_values(['Frequence', 'Longueur'], ascending=[False, True])\n",
    "    \n",
    "    # Statistiques globales\n",
    "    stats = {\n",
    "        'total_lemmes': len(df_vocab),\n",
    "        'freq_min': df_vocab['Frequence'].min(),\n",
    "        'freq_max': df_vocab['Frequence'].max(),\n",
    "        'freq_moyenne': df_vocab['Frequence'].mean(),\n",
    "        'distribution_categories': df_vocab['Categorie'].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Statistiques du vocabulaire:\")\n",
    "    for key, value in stats.items():\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "\n",
    "    # Export des résultats\n",
    "    export_path = './export/vocabulaire_analyse.csv'\n",
    "    df_vocab.to_csv(export_path, index=True, encoding='utf-8')\n",
    "    logger.info(f\"Vocabulaire exporté vers: {export_path}\")\n",
    "    \n",
    "    return df_vocab\n",
    "\n",
    "# Application de la fonction\n",
    "df_vocabulaire = build_vocabulary_table(df)\n",
    "\n",
    "# Affichage des premiers résultats\n",
    "print(\"\\nAperçu du vocabulaire analysé:\")\n",
    "print(df_vocabulaire.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d6222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Export corpus_step_1 réalisé:\n",
      "INFO:__main__:- Fichier: export_corpus_step_1.csv\n",
      "INFO:__main__:- Colonnes exportées: ['uci', 'uce']\n",
      "INFO:__main__:\n",
      "Export corpus réalisé:\n",
      "INFO:__main__:- Fichier: export_corpus.csv\n",
      "INFO:__main__:- Colonnes exportées: ['uci']\n",
      "INFO:__main__:\n",
      "Exports terminés avec succès.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def export_corpus_files(df: pd.DataFrame, export_dir: str = './export') -> None:\n",
    "    \"\"\"\n",
    "    Exporte les fichiers du corpus avec les noms spécifiés\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame à exporter\n",
    "        export_dir (str): Répertoire d'export\n",
    "    \"\"\"\n",
    "    # Création du répertoire d'export s'il n'existe pas\n",
    "    export_path = Path(export_dir)\n",
    "    export_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Configuration des exports avec les noms de fichiers exacts\n",
    "        exports = {\n",
    "            'corpus_step_1': {\n",
    "                'filename': 'export_corpus_step_1.csv',\n",
    "                'columns': ['uci', 'uce']\n",
    "            },\n",
    "            'corpus': {\n",
    "                'filename': 'export_corpus.csv',\n",
    "                'columns': ['uci']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Réalisation des exports\n",
    "        for export_type, export_info in exports.items():\n",
    "            export_file = export_path / export_info['filename']\n",
    "            \n",
    "            # Vérification des colonnes et export\n",
    "            columns_to_export = [col for col in export_info['columns'] if col in df.columns]\n",
    "            if columns_to_export:\n",
    "                df.to_csv(\n",
    "                    export_file,\n",
    "                    columns=columns_to_export,\n",
    "                    index=True,\n",
    "                    encoding='utf-8'\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"\\nExport {export_type} réalisé:\")\n",
    "                logger.info(f\"- Fichier: {export_info['filename']}\")\n",
    "                logger.info(f\"- Colonnes exportées: {columns_to_export}\")\n",
    "            else:\n",
    "                logger.warning(f\"Colonnes manquantes pour l'export {export_type}\")\n",
    "        \n",
    "        logger.info(\"\\nExports terminés avec succès.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'export: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Exécution des exports\n",
    "export_corpus_files(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6074d",
   "metadata": {},
   "source": [
    "# Construction de la matrice TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "160d1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Début de la préparation de la matrice document-terme...\n",
      "INFO:__main__:\n",
      "Statistiques de la matrice:\n",
      "INFO:__main__:- nombre_documents: 196\n",
      "INFO:__main__:- colonnes_presentes: ['uci', 'id_doc', 'uce']\n",
      "INFO:__main__:\n",
      "Aperçu de la matrice préparée:\n",
      "INFO:__main__:\n",
      "                                                 uci  id_doc  \\\n",
      "0  definition opinion publique selon instant pens...       0   \n",
      "1  dirais influence enfin dirais tout simplement ...       1   \n",
      "2  element rappelle rappelle souvent autant taux ...       2   \n",
      "3  pense tout heure pense puisse peut passer apre...       3   \n",
      "4  oui participe webcams rencontres autant redis ...       4   \n",
      "\n",
      "                                                 uce  \n",
      "0  definition opinion publique selon instant pens...  \n",
      "1  dirais influence enfin dirais tout simplement ...  \n",
      "2  element rappelle rappelle souvent autant taux ...  \n",
      "3  pense tout heure pense puisse peut passer apre...  \n",
      "4  oui participe webcams rencontres autant redis ...  \n",
      "INFO:__main__:Matrice document-terme préparée avec succès.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dtm_matrix(export_path: str = './export/export_corpus_step_1.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prépare la matrice document-terme en important le corpus et en ajoutant les IDs\n",
    "    \n",
    "    Args:\n",
    "        export_path (str): Chemin vers le fichier d'export du corpus\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame préparé pour la DTM\n",
    "    \"\"\"\n",
    "    logger.info(\"Début de la préparation de la matrice document-terme...\")\n",
    "    \n",
    "    try:\n",
    "        # Import du corpus avec vérification\n",
    "        if not os.path.exists(export_path):\n",
    "            raise FileNotFoundError(f\"Fichier non trouvé: {export_path}\")\n",
    "            \n",
    "        df_build_dtm = pd.read_csv(export_path, index_col=[0])\n",
    "        \n",
    "        # Vérification des colonnes nécessaires\n",
    "        required_columns = ['uci', 'uce']\n",
    "        missing_columns = [col for col in required_columns if col not in df_build_dtm.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Colonnes manquantes dans le fichier: {missing_columns}\")\n",
    "            \n",
    "        # Ajout de l'ID document de manière optimisée\n",
    "        df_build_dtm.insert(\n",
    "            loc=1,\n",
    "            column=\"id_doc\",\n",
    "            value=range(len(df_build_dtm))\n",
    "        )\n",
    "\n",
    "        # Statistiques sur la matrice\n",
    "        stats = {\n",
    "            'nombre_documents': len(df_build_dtm),\n",
    "            'colonnes_presentes': df_build_dtm.columns.tolist()\n",
    "        }\n",
    "        \n",
    "        logger.info(\"\\nStatistiques de la matrice:\")\n",
    "        for key, value in stats.items():\n",
    "            logger.info(f\"- {key}: {value}\")\n",
    "            \n",
    "        logger.info(\"\\nAperçu de la matrice préparée:\")\n",
    "        logger.info(\"\\n\" + str(df_build_dtm.head()))\n",
    "        \n",
    "        return df_build_dtm\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la préparation de la matrice: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Préparation de la matrice\n",
    "try:\n",
    "    df_build_dtm = prepare_dtm_matrix()\n",
    "    logger.info(\"Matrice document-terme préparée avec succès.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Échec de la préparation de la matrice: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2169dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Début de la création de la matrice terme-document binaire...\n",
      "INFO:__main__:Nombre de termes uniques trouvés: 5214\n",
      "INFO:__main__:\n",
      "Statistiques de la matrice binaire:\n",
      "INFO:__main__:- Dimensions: 196 documents × 5214 termes\n",
      "INFO:__main__:- Nombre total de présences (1): 21008\n",
      "INFO:__main__:- Densité: 2.06%\n",
      "INFO:__main__:- Top 10 termes les plus présents:\n",
      "INFO:__main__:  * donc: présent dans 159 documents\n",
      "INFO:__main__:  * fait: présent dans 150 documents\n",
      "INFO:__main__:  * parce: présent dans 135 documents\n",
      "INFO:__main__:  * tout: présent dans 134 documents\n",
      "INFO:__main__:  * dire: présent dans 125 documents\n",
      "INFO:__main__:  * campagne: présent dans 124 documents\n",
      "INFO:__main__:  * quand: présent dans 113 documents\n",
      "INFO:__main__:  * faire: présent dans 112 documents\n",
      "INFO:__main__:  * peu: présent dans 108 documents\n",
      "INFO:__main__:  * meme: présent dans 104 documents\n",
      "INFO:__main__:Matrice terme-document binaire créée avec succès.\n"
     ]
    }
   ],
   "source": [
    "def create_term_document_matrix(df: pd.DataFrame, text_column: str = 'uce') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Crée une matrice terme-document binaire (0/1) à partir d'un DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame source\n",
    "        text_column (str): Nom de la colonne contenant les textes\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Matrice terme-document avec valeurs binaires (0: absence, 1: présence)\n",
    "    \"\"\"\n",
    "    logger.info(\"Début de la création de la matrice terme-document binaire...\")\n",
    "    \n",
    "    try:\n",
    "        # Vérification des données d'entrée\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Colonne {text_column} non trouvée dans le DataFrame\")\n",
    "        \n",
    "        # Création de la liste des mots uniques\n",
    "        unique_words = []\n",
    "        for text in df[text_column]:\n",
    "            if pd.notna(text) and isinstance(text, str):\n",
    "                words = text.split()\n",
    "                for word in words:\n",
    "                    if word not in unique_words:\n",
    "                        unique_words.append(word)\n",
    "        \n",
    "        if not unique_words:\n",
    "            raise ValueError(\"Aucun mot unique trouvé dans les textes\")\n",
    "            \n",
    "        logger.info(f\"Nombre de termes uniques trouvés: {len(unique_words)}\")\n",
    "\n",
    "        # Initialisation de la matrice avec des zéros (type int)\n",
    "        term_doc_matrix = pd.DataFrame(0, \n",
    "                                     index=df.index,\n",
    "                                     columns=unique_words,\n",
    "                                     dtype=np.int8)  # Utilisation de int8 pour optimiser la mémoire\n",
    "        \n",
    "        # Mise à jour de la matrice avec des 1 pour les présences\n",
    "        for word in unique_words:\n",
    "            term_doc_matrix[word] = df[text_column].apply(\n",
    "                lambda x: 1 if pd.notna(x) and isinstance(x, str) and word in x.split() else 0\n",
    "            ).astype(np.int8)\n",
    "        \n",
    "        # Fusion avec le DataFrame original\n",
    "        result_df = pd.concat([df, term_doc_matrix], axis=1)\n",
    "        \n",
    "        # Statistiques sur la matrice binaire\n",
    "        nb_terms = len(unique_words)\n",
    "        nb_docs = len(df)\n",
    "        presence_count = (term_doc_matrix == 1).sum().sum()\n",
    "        \n",
    "        logger.info(\"\\nStatistiques de la matrice binaire:\")\n",
    "        logger.info(f\"- Dimensions: {nb_docs} documents × {nb_terms} termes\")\n",
    "        logger.info(f\"- Nombre total de présences (1): {presence_count}\")\n",
    "        logger.info(f\"- Densité: {(presence_count / (nb_docs * nb_terms)):.2%}\")\n",
    "        logger.info(\"- Top 10 termes les plus présents:\")\n",
    "        top_terms = term_doc_matrix.sum().nlargest(10)\n",
    "        for term, count in top_terms.items():\n",
    "            logger.info(f\"  * {term}: présent dans {int(count)} documents\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la création de la matrice: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Création de la matrice\n",
    "try:\n",
    "    df_build_dtm = create_term_document_matrix(df_build_dtm)\n",
    "    logger.info(\"Matrice terme-document binaire créée avec succès.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Échec de la création de la matrice: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6988e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Structure de la matrice TDM ===\n",
      "                    Type  Nombre\n",
      "     Colonnes originales       3\n",
      "Termes de la matrice TDM    5214\n",
      "                   Total    5217\n",
      "\n",
      "=== Statistiques détaillées ===\n",
      "Nombre de documents: 196\n",
      "Nombre de termes uniques: 5214\n",
      "Nombre total de présences (1): 21008\n",
      "Densité de la matrice: 2.06%\n",
      "Type de données: {dtype('int8'): 5214}\n",
      "\n",
      "=== Aperçu de la matrice (5 premiers documents, 5 premiers termes) ===\n",
      "   id_doc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   uci                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   uce  definition  opinion  publique  selon  instant\n",
      "0       0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               definition opinion publique selon instant pense public vis vis sujet occurrence opinion publique politique pense comment positionne gens quand dis gens grand public peut etre associations chefs entreprise donc definition donne opinion public comment opinion public mesure taux satisfaction regard question posee                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               definition opinion publique selon instant pense public vis vis sujet occurrence opinion publique politique pense comment positionne gens quand dis gens grand public peut etre associations chefs entreprise donc definition donne opinion public comment opinion public mesure taux satisfaction regard question posee           1        1         1      1        1\n",
      "1       1  dirais influence enfin dirais tout simplement reseaux sociaux digital presents communication politique autant pris part importante communication politique parce comme certains disent faut vivre temps neanmoins faut savoir mesurer veux dire campagne politique evidemment reseaux sociaux evidemment numerique rien mieux aussi contact gens mesurer tout cela dire outil numerique vient complementarite autres outils evidemment rappele pandemie accelere impose modifie relation pouvait avoir puisqu campagne coller affiches contact distribuer tracts marches rencontre individus quel pendant periode covid faire donc dire partie numerique existait pris part importante obligation moment donne neanmoins faut savoir mesurer tout cela autant faut vivre outils numeriques autant faut mettre tete outil numerique outils seul meme aujourd imagine campagne electorale guidee aussi outil numerique  dirais influence enfin dirais tout simplement reseaux sociaux digital presents communication politique autant pris part importante communication politique parce comme certains disent faut vivre temps neanmoins faut savoir mesurer veux dire campagne politique evidemment reseaux sociaux evidemment numerique rien mieux aussi contact gens mesurer tout cela dire outil numerique vient complementarite autres outils evidemment rappele pandemie accelere impose modifie relation pouvait avoir puisqu campagne coller affiches contact distribuer tracts marches rencontre individus quel pendant periode covid faire donc dire partie numerique existait pris part importante obligation moment donne neanmoins faut savoir mesurer tout cela autant faut vivre outils numeriques autant faut mettre tete outil numerique outils seul meme aujourd imagine campagne electorale guidee aussi outil numerique           0        0         0      0        0\n",
      "2       2                                                                                                                                                                                                                                                                                                                                 element rappelle rappelle souvent autant taux abstention quand voit taux aussi eleve taux abstention existait neanmoins accru petit peu vais dire entrer contact parce arrive entrer contact gens neanmoins opinion publique desinteret grandissant concernant politique alors assez paradoxal parce quand rencontre gens disent politique interesse sujets politiques interessent guillemets personnel politique peut dire ainsi donc besoin pays interesse beaucoup chose publique voit neanmoins comment redonne sens politique sens large parce fonction politique decredibilisee                                                                                                                                                                                                                                                                                                                                 element rappelle rappelle souvent autant taux abstention quand voit taux aussi eleve taux abstention existait neanmoins accru petit peu vais dire entrer contact parce arrive entrer contact gens neanmoins opinion publique desinteret grandissant concernant politique alors assez paradoxal parce quand rencontre gens disent politique interesse sujets politiques interessent guillemets personnel politique peut dire ainsi donc besoin pays interesse beaucoup chose publique voit neanmoins comment redonne sens politique sens large parce fonction politique decredibilisee           0        1         1      0        0\n",
      "3       3                                                                                                                                            pense tout heure pense puisse peut passer apres comment fait vivre parce site campagne essentiel important faut vivre temps neanmoins rien pire avoir site campagne vivant quand dis vivant actualise permette lien parce toujours lien campagne electorale lien lien distance lien presentiel lien physique neanmoins mettre site ligne campagne obligation reel besoin neanmoins faut quelqu alimenter faire vivre donc faut trompe quand fait site campagne fois vaut mieux rien avoir avoir site vit parce politique vie aussi relation echanges echange fait besoin etre nourri evidemment site alors site outils numeriques essentiels aujourd mener campagnes neanmoins oublier faire vivre sites alimenter nourrir sites interactifs permettent contact population                                                                                                                                            pense tout heure pense puisse peut passer apres comment fait vivre parce site campagne essentiel important faut vivre temps neanmoins rien pire avoir site campagne vivant quand dis vivant actualise permette lien parce toujours lien campagne electorale lien lien distance lien presentiel lien physique neanmoins mettre site ligne campagne obligation reel besoin neanmoins faut quelqu alimenter faire vivre donc faut trompe quand fait site campagne fois vaut mieux rien avoir avoir site vit parce politique vie aussi relation echanges echange fait besoin etre nourri evidemment site alors site outils numeriques essentiels aujourd mener campagnes neanmoins oublier faire vivre sites alimenter nourrir sites interactifs permettent contact population           0        0         0      0        0\n",
      "4       4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              oui participe webcams rencontres autant redis autant situation sanitaire laquelle ete accelere participe maniere generale communication evidemment faut etre lien outils outils numeriques existent tres principalement presse regionale locale aussi outils numeriques donc participe cela autre part interne reunions campagne reunions politiques fait genre echanges                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              oui participe webcams rencontres autant redis autant situation sanitaire laquelle ete accelere participe maniere generale communication evidemment faut etre lien outils outils numeriques existent tres principalement presse regionale locale aussi outils numeriques donc participe cela autre part interne reunions campagne reunions politiques fait genre echanges           0        0         0      0        0\n",
      "\n",
      "=== Top 10 termes les plus fréquents ===\n",
      "- donc: présent dans 159 documents\n",
      "- fait: présent dans 150 documents\n",
      "- parce: présent dans 135 documents\n",
      "- tout: présent dans 134 documents\n",
      "- dire: présent dans 125 documents\n",
      "- campagne: présent dans 124 documents\n",
      "- quand: présent dans 113 documents\n",
      "- faire: présent dans 112 documents\n",
      "- peu: présent dans 108 documents\n",
      "- meme: présent dans 104 documents\n"
     ]
    }
   ],
   "source": [
    "def display_tdm_structure(df: pd.DataFrame, original_columns: list) -> None:\n",
    "    \"\"\"\n",
    "    Affiche la structure détaillée de la matrice TDM\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant la matrice TDM\n",
    "        original_columns (list): Liste des colonnes d'origine avant ajout de la matrice\n",
    "    \"\"\"\n",
    "    # Séparation des colonnes originales et de la matrice TDM\n",
    "    tdm_columns = [col for col in df.columns if col not in original_columns]\n",
    "    \n",
    "    # Création du DataFrame de structure\n",
    "    structure_info = pd.DataFrame({\n",
    "        'Type': [\n",
    "            'Colonnes originales',\n",
    "            'Termes de la matrice TDM',\n",
    "            'Total'\n",
    "        ],\n",
    "        'Nombre': [\n",
    "            len(original_columns),\n",
    "            len(tdm_columns),\n",
    "            len(df.columns)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Calcul des statistiques de la matrice TDM\n",
    "    tdm_part = df[tdm_columns]\n",
    "    tdm_stats = {\n",
    "        'Nombre de documents': len(df),\n",
    "        'Nombre de termes uniques': len(tdm_columns),\n",
    "        'Nombre total de présences (1)': (tdm_part == 1).sum().sum(),\n",
    "        'Densité de la matrice': f\"{((tdm_part == 1).sum().sum() / (len(df) * len(tdm_columns))):.2%}\",\n",
    "        'Type de données': tdm_part.dtypes.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(\"\\n=== Structure de la matrice TDM ===\")\n",
    "    print(structure_info.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Statistiques détaillées ===\")\n",
    "    for key, value in tdm_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n=== Aperçu de la matrice (5 premiers documents, 5 premiers termes) ===\")\n",
    "    sample_cols = original_columns + tdm_columns[:5]\n",
    "    print(df[sample_cols].head().to_string())\n",
    "    \n",
    "    print(\"\\n=== Top 10 termes les plus fréquents ===\")\n",
    "    top_terms = tdm_part.sum().nlargest(10)\n",
    "    for term, count in top_terms.items():\n",
    "        print(f\"- {term}: présent dans {int(count)} documents\")\n",
    "\n",
    "# Après la création de la matrice TDM\n",
    "original_columns = ['id_doc', 'uci', 'uce']  # Ajustez selon vos colonnes d'origine\n",
    "display_tdm_structure(df_build_dtm, original_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba70a574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Préparation de la matrice pour l'analyse Reinert...\n",
      "INFO:__main__:Matrice exportée vers: ./export/reinert_dataframe.csv\n",
      "INFO:__main__:\n",
      "Structure de la matrice Reinert:\n",
      "INFO:__main__:- Dimensions: (196, 5216)\n",
      "INFO:__main__:- Nombre de termes: 5214\n",
      "INFO:__main__:- Nombre de documents: 196\n",
      "INFO:__main__:\n",
      "Vérification de la cohérence:\n",
      "INFO:__main__:- Types de données: {dtype('int64'): 5214}\n",
      "INFO:__main__:- Valeurs manquantes: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu de la matrice finale:\n",
      "        definition  opinion  publique  selon  instant  pense  public  vis  \\\n",
      "id_doc                                                                      \n",
      "0                1        1         1      1        1      1       1    1   \n",
      "1                0        0         0      0        0      0       0    0   \n",
      "2                0        1         1      0        0      0       0    0   \n",
      "3                0        0         0      0        0      1       0    0   \n",
      "4                0        0         0      0        0      0       0    0   \n",
      "\n",
      "        sujet  occurrence  ...  directions  ressemblent  mastodontes  usages  \\\n",
      "id_doc                     ...                                                 \n",
      "0           1           1  ...           0            0            0       0   \n",
      "1           0           0  ...           0            0            0       0   \n",
      "2           0           0  ...           0            0            0       0   \n",
      "3           0           0  ...           0            0            0       0   \n",
      "4           0           0  ...           0            0            0       0   \n",
      "\n",
      "        foutrac  adaptabilite  proposees  plonger  anglais  chaise  \n",
      "id_doc                                                              \n",
      "0             0             0          0        0        0       0  \n",
      "1             0             0          0        0        0       0  \n",
      "2             0             0          0        0        0       0  \n",
      "3             0             0          0        0        0       0  \n",
      "4             0             0          0        0        0       0  \n",
      "\n",
      "[5 rows x 5214 columns]\n"
     ]
    }
   ],
   "source": [
    "def prepare_reinert_matrix(df: pd.DataFrame, export_path: str = './export/reinert_dataframe.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prépare et exporte la matrice pour l'analyse Reinert\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame source (matrice TDM)\n",
    "        export_path (str): Chemin pour l'export du fichier\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Matrice préparée pour l'analyse Reinert\n",
    "    \"\"\"\n",
    "    logger.info(\"Préparation de la matrice pour l'analyse Reinert...\")\n",
    "    \n",
    "    try:\n",
    "        # Vérification des colonnes requises\n",
    "        required_columns = ['id_doc', 'uci', 'uce']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Colonnes manquantes: {missing_columns}\")\n",
    "        \n",
    "        # Création de la matrice Reinert\n",
    "        df_reinert = df.drop(columns=['uce'])\n",
    "        \n",
    "        # Vérification de la structure avant export\n",
    "        matrix_stats = {\n",
    "            'dimensions': df_reinert.shape,\n",
    "            'colonnes': len(df_reinert.columns) - 2,  # -2 pour id_doc et uci\n",
    "            'documents': len(df_reinert)\n",
    "        }\n",
    "\n",
    "        # Export de la matrice\n",
    "        export_dir = os.path.dirname(export_path)\n",
    "        if not os.path.exists(export_dir):\n",
    "            os.makedirs(export_dir)\n",
    "            \n",
    "        df_reinert.to_csv(export_path, index=False)\n",
    "        logger.info(f\"Matrice exportée vers: {export_path}\")\n",
    "        \n",
    "        # Chargement et préparation finale\n",
    "        df_reinert_afc = pd.read_csv(export_path, header=0, index_col=0)\n",
    "        df_reinert_afc = df_reinert_afc.set_index('id_doc')\n",
    "        \n",
    "        # Affichage des informations\n",
    "        logger.info(\"\\nStructure de la matrice Reinert:\")\n",
    "        logger.info(f\"- Dimensions: {matrix_stats['dimensions']}\")\n",
    "        logger.info(f\"- Nombre de termes: {matrix_stats['colonnes']}\")\n",
    "        logger.info(f\"- Nombre de documents: {matrix_stats['documents']}\")\n",
    "        \n",
    "        # Vérification de la cohérence\n",
    "        logger.info(\"\\nVérification de la cohérence:\")\n",
    "        logger.info(f\"- Types de données: {df_reinert_afc.dtypes.value_counts().to_dict()}\")\n",
    "        logger.info(f\"- Valeurs manquantes: {df_reinert_afc.isnull().sum().sum()}\")\n",
    "        \n",
    "        return df_reinert_afc\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la préparation de la matrice Reinert: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Exécution de la préparation\n",
    "try:\n",
    "    df_reinert_afc = prepare_reinert_matrix(df_build_dtm)\n",
    "    \n",
    "    # Affichage de l'aperçu\n",
    "    print(\"\\nAperçu de la matrice finale:\")\n",
    "    print(df_reinert_afc.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Échec de la préparation: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
